---
layout: post
title: "HIVE的HQL中调用python脚本"
image: http://ww3.sinaimg.cn/large/005yyi5Jjw1eoerczxex0j30im08ltau.jpg
categories: JsPy&Others
date: 2015-12-17
---


一点技术上的积累,在HIVE中调用python来完成Reduce的方式

HIVE从技术上来说,可以看做是由JAVA打包好的Mapreduce函数,而在reduce
的过程中,很多事情不能简单的由HIVE中的函数实现,也有多种解决的途径:

1. UDF UserDefinedFunction
2. 写JAVA的Reduce
3. 使用Streaming来调用其他形式的脚本来实现复杂的计算

这里简单介绍第三种方式,使用脚本(比如Python)来实现比较复杂的计算
需要注意的是:

1. 要保证在HIVE的每个节点上都安装好了Python脚本中相关的包
2. Python代码中使用sys.stdin输入,\t分割,print输出(如果熟悉hadoop streaming的流程,编写起来应该会很快)
3. 在HIVE的HQL语句中,使用ADD FILE来加载路径中的Python文件
4. 使用TRANSFORM和USING来调用脚本实现计算


Python代码部分:读入数据,并将第二,第三列粘贴到一起,作为输出(和HIVE中的`concat`类似)

    #!/usr/bin/env python
    # -*- coding:utf-8 -*-
    import sys
    for line in sys.stdin:
            try:
                    line = line.strip()
                    li = line.split('\t')
                    new = li[1]+li[2]
                    print new.strip()
            except Exception,err:
                    continue


以下代码是HIVE脚本,选取某个表,并将该表中的二,三列粘贴到一起.

    ADD FILE mapper.py;
    SELECT TRANSFORM(*)
    USING 'python mapper.py'
    AS aaa
    FROM mytable limit 5
    ;



最后说下该调用方式的优劣势:

优势:

1. 对于不熟悉JAVA的人来说,这样写Py还是方便些
2. 从某种程度上来说,相当于使用HIVE实现了Python或其他脚本的并行化.

劣势:

1. 需要在每个节点上安装所需要的包
